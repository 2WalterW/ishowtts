# Performance Analysis - 2025-09-30

## Current State Summary

**Achievement**: RTF = 0.169 (mean), 0.165 (best) with NFE=7 ✅
**Status**: Phase 3 COMPLETE - All targets exceeded

## Code Review Findings

### 1. Already Applied Optimizations
- ✅ torch.compile(mode='max-autotune') for model and vocoder
- ✅ Automatic Mixed Precision (FP16) via torch.amp.autocast
- ✅ Reference audio tensor caching
- ✅ CUDA stream optimization for async transfers
- ✅ NFE reduced to 7 (from 32)
- ✅ GPU frequency locking for stability
- ✅ Skip spectrogram generation when not needed

### 2. Identified Optimization Opportunities

#### A. Vocoder Decode Outside Autocast Context (POTENTIAL BUG)
**File**: `third_party/F5-TTS/src/f5_tts/infer/utils_infer.py:548-568`

**Issue**: Vocoder decode is inside FP16 autocast, but then output is converted back to FP32.
This means the vocoder benefits from FP16, but there's an unnecessary conversion.

**Current code**:
```python
with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
    generated, _ = model_obj.sample(...)  # Model in FP16 ✅
    generated = generated.to(torch.float32)  # Convert to FP32
    generated = generated[:, ref_audio_len:, :]
    generated = generated.permute(0, 2, 1)
    if mel_spec_type == "vocos":
        generated_wave = vocoder.decode(generated)  # Vocoder with FP32 input ❓
```

**Analysis**: The vocoder is compiled with torch.compile, so it should stay in FP16 context.
However, we're converting the mel spectrogram back to FP32 before feeding it to the vocoder.

**Two options**:
1. Keep vocoder in FP16 context (current approach, but with unnecessary conversion)
2. Move vocoder outside autocast and use FP32 (if quality is concern)

**Recommendation**: Keep everything in FP16 until final output.

#### B. Empty Cache Call May Be Harmful
**File**: `third_party/F5-TTS/src/f5_tts/infer/utils_infer.py:594-596`

```python
# GPU memory optimization: Clear intermediate tensors
del generated
if device and "cuda" in str(device):
    torch.cuda.empty_cache()
```

**Issue**: `torch.cuda.empty_cache()` can cause performance degradation:
- It's a synchronization point that blocks the CUDA stream
- PyTorch's caching allocator is very efficient
- Only needed if actually running out of memory
- Can cause 5-10ms overhead per call

**Recommendation**: Remove `empty_cache()` unless memory pressure is detected.

#### C. Redundant RMS Calculation in Cache Hit Path
**File**: `third_party/F5-TTS/src/f5_tts/infer/utils_infer.py:493-519`

**Issue**: When cache is hit, `rms` is set to `target_rms` (line 496), but later code assumes it's the actual RMS (lines 567-568, 587-588).

```python
if rms < target_rms:
    generated_wave = generated_wave * rms / target_rms
```

This condition will never be true if cache was hit (rms == target_rms), so the adjustment is skipped.

**Impact**: Potential audio level inconsistency between cached and non-cached calls.

**Recommendation**: Cache the actual RMS value along with the audio tensor.

#### D. Potential Batch Processing Optimization
**Current**: Sequential processing of batches
**Opportunity**: True batch processing could amortize overhead

However, the current API processes one generation at a time, so this would require API changes.

#### E. Unnecessary Type Conversions
**File**: Multiple locations in utils_infer.py

**Observation**: Multiple `.to(torch.float32)` conversions that might be avoidable if we maintain FP16 throughout.

### 3. Recommended Next Steps (Priority Order)

#### Priority 1: Fix Vocoder FP16 Consistency (Quick Win)
**Estimated Impact**: 5-10% speedup (RTF 0.169 → 0.152-0.160)
**Risk**: Low (easy to test and revert)
**Effort**: 15-30 minutes

**Action**: Keep mel spectrogram in FP16, pass to vocoder in FP16, only convert final waveform to FP32.

#### Priority 2: Remove torch.cuda.empty_cache() (Quick Win)
**Estimated Impact**: 2-5% speedup (saves 5-10ms per inference)
**Risk**: Very low (only matters if OOM)
**Effort**: 5 minutes

**Action**: Comment out or remove the empty_cache call.

#### Priority 3: Fix RMS Caching Logic (Correctness)
**Estimated Impact**: Quality consistency (no speed impact)
**Risk**: Very low
**Effort**: 10-15 minutes

**Action**: Store actual RMS in cache, not just target_rms.

#### Priority 4: Profile and Identify Remaining Bottlenecks
**Estimated Impact**: Unknown (depends on findings)
**Risk**: None (just profiling)
**Effort**: 30-60 minutes

**Action**: Use PyTorch profiler to identify hotspots.

### 4. Longer-Term Optimizations (Future Work)

#### A. NFE=6 Testing
**Potential**: 14% speedup (RTF ~0.145)
**Risk**: Quality degradation
**Effort**: Quality evaluation needed

#### B. INT8 Quantization
**Potential**: 1.5-2x speedup (RTF ~0.08-0.11)
**Risk**: Quality degradation
**Effort**: 2-4 weeks (calibration, testing)

#### C. CUDA Graphs
**Potential**: 10-15% speedup for fixed shapes
**Risk**: Complex to implement, limited applicability
**Effort**: 1-2 weeks

#### D. Model Architecture Changes
**Potential**: Significant, but requires research
**Risk**: High (model retraining)
**Effort**: 4-8 weeks

## Summary

The codebase is already heavily optimized. The main quick wins are:
1. **Fix FP16 consistency in vocoder path** (5-10% gain)
2. **Remove empty_cache()** (2-5% gain)
3. **Fix RMS caching** (correctness)

Combined potential: RTF 0.169 → **0.143-0.155** (10-15% improvement)

This would bring us very close to the stretch goal of RTF < 0.15!